# 实验报告 lab1

## 环境配置

在软件工程课已配置好mysql的环境，开启MySQL服务后，命令行输入以下指令即可启动

```bash
mysql -u root -p
```

## 实验步骤

### 1. python mysql库使用

Python 的sql库主要由connection和cursor实现

* Connection维持与数据库的连接，处理提交以及异常等事项
* Cursor游标用于执行原始的sql语句并返回结果
* 其他内容包括提高性能的连接池和防止注入的措施

### 2. csv数据的读取和清洗

使用pandas库提供的方法直接处理csv

```python
def read_csv(path:str,encoding="utf-8")->pd.DataFrame:
    data:pd.DataFrame=pd.read_csv(path,encoding=encoding)
    return data
```

并可以通过pd的接口直接根据主键去重，例如

```python
df = df.drop_duplicates(subset=filter_subset,keep="first")  
```

对数据的清洗可以有效防止复杂的sql语句，在本次lab中存在少量重复数据

### 3. 建立数据表结构

我们直接通过库的接口读取原始sql文件创建表。首先会抛弃旧的表并建立一个新表

```sql
SET GLOBAL sql_mode=STRICT_TRANS_TABLES;



DROP TABLE IF EXISTS room_info;
DROP TABLE IF EXISTS student_info;

CREATE TABLE student_info (
  registration_number INT NOT NULL,
  name VARCHAR(50) NOT NULL,
  exam_center_id INT,
  exam_room_id INT,
  session_id INT,
  seat INT,
  PRIMARY KEY (registration_number)
);




CREATE TABLE room_info(
    exam_center_id INT NOT NULL,
    exam_room_id INT NOT NULL,
    session_id INT NOT NULL,
    exam_center_name VARCHAR(40),
    expected_time VARCHAR(40),
    paper_name VARCHAR(40),
    PRIMARY KEY (exam_center_id, exam_room_id, session_id)
);

```

这里的column名和csv不完全相同，需要额外的mapping进行映射，会在之后说明  

以下python代码会根据sql文件执行语句创建表

```python
def create_table(self, sql_file_path: str):
    try:
        with open(sql_file_path, 'r', encoding='utf-8') as file:
            sql_statements = file.read()
        self.cursor.execute(sql_statements)
        self.conn.commit()
        print("Table created successfully")
    except Exception as e:
        print("Error creating table:", e)
```

### 4.向表中插入数据

```python
try:
    df = read_csv(csv_file_path,encoding=encoding)
    columns = list(df.columns)
    placeholders = ', '.join(['%s'] * len(columns))
    if mapping == None:
        col_names = ', '.join(columns)
    else:
        col_names = ', '.join([mapping.get(col, col) for col in columns])
    
    insert_sql_template = f"INSERT INTO {table_name} ({col_names}) VALUES ({placeholders})"
    
    filter_subset=primary_keys if isinstance(primary_keys,list) else [primary_keys]
    
    df = df.drop_duplicates(subset=filter_subset, keep="first")  
    
    if df.empty:
        print("All data already exists in the database. No insertion needed.")
        return
    
    for index, row in df.iterrows():
        values = tuple(None if pd.isna(x) else str(x) for x in row)
        self.cursor.execute(insert_sql_template,values)
        
    self.conn.commit()
    print(f"Data inserted successfully to table {table_name}")
except Exception as e:
    print(f"Error inserting data to table {table_name}:", e)
    traceback.print_exc()
```

在这里，需要注意的就是使用mapping将csv和数据库里的列名映射对应，从而使得数据库的描述更清晰。 
在这里使用了基本的INSERT INTO 语法，并根据参数动态填充表名、列名以及values
values由column数量个%s组成，并在实际执行的时候额外传入数据元组。这样的好处是可以防止sql注入。

也可以用手动将Values用python字符串处理为values相连的字符串，但需要注意到字符串类型需要额外加上''。

在这里，每个循环执行的语句类似如下(以student_info为例)

```sql
INSERT INTO student_info 
(registration_number, name,exam_center_id, exam_room_id, session_id, seat)
VALUEs (110011, 'name_of_stu', 1, 1, 1, 1)
```

该lab的主要实验内容到此结束

## 思考部分

### 原始数据不完整

个人认为可能的解决方案如下：

* 在插入前可以先对数据进行清洗，手动调整缺失项，避免影响业务。适用于数据较为重要的情况。
* 在插入时提供默认值或缺省表示值，在之后处理的时候可以标识不完整的数据，适用于大规模的数据。
* 直接删除缺失数据，适用于单条数据不是非常重要的情况。
  
### 原始数据有不一致

* 操作前先查找不符合要求的数据并手动补全或填充缺省值，参考不完整的解决方法，保证操作本身的健壮
* 同样可以直接删除不重要的不一致项
* 减少对单一约束的依赖

### 思考总结

* 自己的数据库操作方法保证健壮性，避免因为少量脏数据无法执行
* 操作前提前对数据进行筛选和清洗，按需求处理可能的问题
* 数据使用前使用外部方法先标准化，避免数据库内部的大量操作

## 总结

本次lab使用了基本的sql语句和高级语言的数据库操作库，读入了外部数据并在基本处理后写入了数据库，整体难度不大。